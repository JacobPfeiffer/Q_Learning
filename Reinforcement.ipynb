{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning \n",
    "For this project I will be using Q-learning, a form of temporal differencing to find the optimal path an agent should take to make it to an exit in a grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 0.25], [0, 0, 0, 0], [0, 0.25, 0.25, 0], [0, 0, 0, 0], [0, 0, 0, 0.25], [0, 0, 0, 500], [0, 0.25, 0.25, 0], [0, 0, 500, 0], [0.25, 0.25, 0, 0.25], [0, 0, 0, 0], [0, -1, 0.25, 0], [0, 0, 0, 0], [0.25, 0, 0, 0.25], [500, 0, 0, 0], [0, 0, 0.25, 0.25], [0, 0, 0, 0], [0.25, 0, 0.25, -1], [0, 0, 0, 0], [0, 0, -1, 0], [0, 0, 0, 0], [0.25, 0, 0, 0]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "rows = 3\n",
    "columns = 7\n",
    "up = 0\n",
    "down = 1\n",
    "left = 2\n",
    "right = 3\n",
    "actions = [up,down,left,right]\n",
    "\n",
    "# O is an open tile\n",
    "# T is a treasure tile\n",
    "# X is an obstacle tile\n",
    "# E is an exit tile\n",
    "# H is an enemy henchmen tile\n",
    "# S is a start tile\n",
    "\n",
    "# Gridworld\n",
    "\n",
    "# OTXHXTE\n",
    "# OOTOOOT\n",
    "# STOHOOO\n",
    "\n",
    "# agent can move up, down, left, and right within the grid\n",
    "# there are 49 states/tiles\n",
    "grid = [['O','T','X','X','X','T','E'],['O','O','T','O','O','O','T'],['S','T','O','H','X','O','O']]\n",
    "\n",
    "#initialize a grid of reward/penalties corresponding to each grid position\n",
    "# -1 penalty for moving to H tile\n",
    "# +0.25 for moving to T tile\n",
    "# +500 for moving to E tile\n",
    "# If agent tries to move into an obstacle they will end up back in the tile they tried to move from. Hence no change\n",
    "# wraps around like packman to the left and right but not up and down\n",
    "i=0\n",
    "reward = []\n",
    "for r in range(rows):\n",
    "    for c in range(columns):\n",
    "        reward.append(list())\n",
    "        for a in range(len(actions)):\n",
    "            if a == up:\n",
    "                row = r-1\n",
    "                column = c\n",
    "                if row<0 or column<0 or row>=rows or column>=columns:\n",
    "                    reward[i].append(0)\n",
    "                elif grid[row][column]=='O' or grid[row][column]=='X' or grid[row][column]=='S':\n",
    "                    reward[i].append(0)\n",
    "                elif grid[row][column]=='H':\n",
    "                    reward[i].append(-1)\n",
    "                elif grid[row][column]=='T':\n",
    "                    reward[i].append(.25) \n",
    "                elif grid[row][column]=='E':\n",
    "                    reward[i].append(500)\n",
    "            elif a == down:\n",
    "                row = r+1\n",
    "                column = c\n",
    "                if row<0 or column<0 or row>=rows or column>=columns:\n",
    "                    reward[i].append(0)\n",
    "                elif grid[row][column]=='O' or grid[row][column]=='X' or grid[row][column]=='S':\n",
    "                    reward[i].append(0)\n",
    "                elif grid[row][column]=='H':\n",
    "                    reward[i].append(-1)\n",
    "                elif grid[row][column]=='T':\n",
    "                    reward[i].append(.25) \n",
    "                elif grid[row][column]=='E':\n",
    "                    reward[i].append(500)\n",
    "            elif a == left:\n",
    "                row = r\n",
    "                column = c-1\n",
    "                if column < 0 and r>0:\n",
    "                    row = r-1\n",
    "                    column = columns-1\n",
    "                if row<0 or column<0 or row>=rows or column>=columns:\n",
    "                    reward[i].append(0)\n",
    "                elif grid[row][column]=='O' or grid[row][column]=='X' or grid[row][column]=='S':\n",
    "                    reward[i].append(0)\n",
    "                elif grid[row][column]=='H':\n",
    "                    reward[i].append(-1)\n",
    "                elif grid[row][column]=='T':\n",
    "                    reward[i].append(.25) \n",
    "                elif grid[row][column]=='E':\n",
    "                    reward[i].append(500)\n",
    "            elif a == right:\n",
    "                row = r\n",
    "                column = c+1\n",
    "                if column > columns-1 and r<rows-1:\n",
    "                    row = r+1\n",
    "                    column = 0\n",
    "                if row<0 or column<0 or row>=rows or column>=columns:\n",
    "                    reward[i].append(0)\n",
    "                elif grid[row][column]=='O' or grid[row][column]=='X' or grid[row][column]=='S':\n",
    "                    reward[i].append(0)\n",
    "                elif grid[row][column]=='H':\n",
    "                    reward[i].append(-1)\n",
    "                elif grid[row][column]=='T':\n",
    "                    reward[i].append(.25) \n",
    "                elif grid[row][column]=='E':\n",
    "                    reward[i].append(500)  \n",
    "        i=i+1\n",
    "\n",
    "print(reward)\n",
    "\n",
    "# create Q-table that will be updated on each iteration\n",
    "Q_table = np.zeros((len(reward),len(reward[0])))\n",
    "print(Q_table)\n",
    "\n",
    "# TODO make a penalty for each move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[450.7 474.6 450.7 428.2]\n",
      " [427.9 450.7 450.7 427.9]\n",
      " [  0.    0.    0.    0. ]\n",
      " [  0.    0.    0.    0. ]\n",
      " [  0.    0.    0.    0. ]\n",
      " [461.2 434.2 461.  499.8]\n",
      " [  0.    0.    0.    0. ]\n",
      " [450.2 450.6 499.8 449.1]\n",
      " [428.2 428.2 474.6 428.2]\n",
      " [427.9 406.6 450.7 406.6]\n",
      " [378.7 326.3 428.2 368.1]\n",
      " [417.8 378.  382.3 441.9]\n",
      " [474.9 415.3 386.5 468.8]\n",
      " [499.8 450.9 450.9 450.9]\n",
      " [474.6 450.9 474.9 428.2]\n",
      " [450.7 427.2 450.1 404.7]\n",
      " [428.2 353.9 403.4 331.7]\n",
      " [403.4 150.7 135.7 100.8]\n",
      " [  0.    0.    0.    0. ]\n",
      " [450.5 183.9 195.6 229.8]\n",
      " [474.9 420.  400.5 426.4]]\n"
     ]
    }
   ],
   "source": [
    "def calcPosition(current_pos, action):\n",
    "    if action == up:\n",
    "        pos = current_pos-7\n",
    "    elif action == down:\n",
    "        pos = current_pos+7\n",
    "    elif action == right:\n",
    "        pos = current_pos+1\n",
    "    else:\n",
    "        pos = current_pos-1\n",
    "    return pos\n",
    "\n",
    "\n",
    "# Q-Learning alg\n",
    "iterations = 10000\n",
    "epsilon = 0.3\n",
    "alpha = 0.05\n",
    "gamma = .95\n",
    "act =0\n",
    "start_pos = 14\n",
    "position = 14\n",
    "\n",
    "for it in range(iterations):\n",
    "    cont = True\n",
    "    position = start_pos\n",
    "    while cont:\n",
    "        if epsilon > np.random.rand():\n",
    "            act = np.random.choice(actions)\n",
    "        else:\n",
    "            act = np.argmax(reward[position]) \n",
    "        # TODO check if calc position is correct\n",
    "        new_pos = calcPosition(position,act)\n",
    "        # check for valid position\n",
    "        r=new_pos//7\n",
    "        c=new_pos%7\n",
    "        if new_pos<0 or new_pos>=len(Q_table) or grid[r][c]=='X':\n",
    "            new_pos = position\n",
    "        r=new_pos//7\n",
    "        c=new_pos%7\n",
    "        # 0.2 penalty on each movement seen as subtracting from the reward\n",
    "        Q_table[position][act] += alpha*(reward[position][act]-0.2+ gamma*np.max(Q_table[new_pos])-Q_table[position][act])\n",
    "        position = new_pos\n",
    "        if grid[r][c]=='E':\n",
    "            cont = False\n",
    "# round q_table\n",
    "Q_table = np.array(Q_table)\n",
    "Q_table = Q_table.round(1)    \n",
    "print(Q_table)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Left', 'Up']\n"
     ]
    }
   ],
   "source": [
    "optimal_path = list()\n",
    "\n",
    "exit = True\n",
    "position = 14\n",
    "while exit:\n",
    "    direction = np.argmax(Q_table[position])\n",
    "    \n",
    "    if direction == up:\n",
    "        optimal_path.append('Up')\n",
    "    elif direction == down:\n",
    "        optimal_path.append('Down')\n",
    "    elif direction == left:\n",
    "        optimal_path.append('Left')\n",
    "    elif direction == right:\n",
    "        optimal_path.append('Right')\n",
    "    position = calcPosition(position,direction)\n",
    "    if position == 6:\n",
    "            break\n",
    "print(optimal_path)\n",
    "# Gridworld\n",
    "# OTXHXTE\n",
    "# OOTOOOT\n",
    "# STOHOOO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is in fact optimal. If the agent moves left, it will wrap around and end up a row higher at the rightmost column. There is a treasure in this cell. From there, if the agent moves up it will reach the exit. Due to the penalty for each move, this solution is optimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
